{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e515f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188af113",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../../data/meditations.txt\"\n",
    "PRE_TRAINED_MODEL_PATH = \"../../models/meditations_mlm_bert_base_uncased_continual_pretraining\"\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 2\n",
    "MAX_LENGTH = 256\n",
    "SHUFFLE    = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b591df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATASET_PATH, \"r\") as fin:\n",
    "    text = fin.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae2b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mlm_dataset(tokenizer, inputs):\n",
    "    # create labels tensor by cloning the input_ids tensor\n",
    "    inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "    \n",
    "    # create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(inputs.input_ids.shape)\n",
    "    \n",
    "    # create mask array except for special tokens [CLS], [SEP], [PAD]\n",
    "    mask_arr = (rand < 0.15) \\\n",
    "             * (inputs.input_ids != 101) \\\n",
    "             * (inputs.input_ids != 102) \\\n",
    "             * (inputs.input_ids != 0)\n",
    "    \n",
    "    selection = []\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "    \n",
    "    # For each row in the input_ids, assign 103 [MASK] token to the selection indices\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        inputs.input_ids[i, selection[i]] = 103\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d391c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {key: torch.tensor(value[index].clone().detach()) for key, value in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15940d3f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:17: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370116979/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "#\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# tokenize the text\n",
    "inputs = tokenizer(text,\n",
    "                   return_tensors='pt',\n",
    "                   max_length=MAX_LENGTH,\n",
    "                   truncation=True,\n",
    "                   padding='max_length')\n",
    "\n",
    "inputs = generate_mlm_dataset(tokenizer, inputs)\n",
    "\n",
    "#\n",
    "dataset = MLMDataset(inputs)\n",
    "loader  = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb23d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # this just creates blank progress bars for both our epochs\n",
    "    loop = tqdm(loader, leave=True) #leave=True enables us to see the progress bars for each epoch\n",
    "    for batch in loop:\n",
    "        # zero_grad sets the gradients of all optimized tensors to zero\n",
    "        # we need to set the gradients to zero b/f starting backprop b/c pytoch accumulates the gradients on subsequent backward pass (this is convenient while training RNNs)\n",
    "        # \n",
    "        optimizer.zero_grad() # stops the gradient calculations from the previous set being carried over to the next set\n",
    "        \n",
    "        # batch in loop contains the 4 inputs (ie. input_ids, token_ids, attention_masks and labels)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # forward pass, feeding input data through all the neurons in the network from first to last layer\n",
    "        outputs = model(input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward propagation, compute the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # update model parameters and take a step using the computerd gradient\n",
    "        optimizer.step()\n",
    "        loop.set_description(\"Epoch {}\".format(epoch))\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "model.save_pretrained(PRE_TRAINED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c582d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
