{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "565fff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbacf7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../../data/meditations.txt\"\n",
    "PRE_TRAINED_MODEL_PATH = \"../../models/meditations_mlm_nsp_bert_base_uncased_continual_pretraining\"\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 2\n",
    "MAX_LENGTH = 256\n",
    "SHUFFLE    = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4207154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nsp_dataset(text):\n",
    "    # bag of sentences\n",
    "    bag = [item for sentence in text for item in sentence.split('.') if item != '']\n",
    "    bag_size = len(bag)\n",
    "\n",
    "    sentence_a = []\n",
    "    sentence_b = []\n",
    "    label = []\n",
    "\n",
    "    for paragraph in text:\n",
    "        sentences = [\n",
    "            sentence for sentence in paragraph.split('.') if sentence != ''\n",
    "        ]\n",
    "        num_sentences = len(sentences)\n",
    "        if num_sentences > 1:\n",
    "            start = random.randint(0, num_sentences-2)\n",
    "            # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "            # In labels,\n",
    "                # 0 indicates sequence B is a continuation of sequence A,\n",
    "                # 1 indicates sequence B is a random sequence.\n",
    "            \n",
    "            if random.random() >= 0.5:\n",
    "                # this is IsNextSentence\n",
    "                sentence_a.append(sentences[start])\n",
    "                sentence_b.append(sentences[start+1])\n",
    "                label.append(0)\n",
    "            else:\n",
    "                index = random.randint(0, bag_size-1)\n",
    "                # this is NotNextSentence\n",
    "                sentence_a.append(sentences[start])\n",
    "                sentence_b.append(bag[index])\n",
    "                label.append(1)\n",
    "    \n",
    "    return sentence_a, sentence_b, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "505d2612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mlm_dataset(tokenizer, inputs):\n",
    "    \n",
    "    # create labels tensor by cloning the input_ids tensor\n",
    "    inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "    \n",
    "    # create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(inputs.input_ids.shape)\n",
    "    \n",
    "    # create mask array except for special tokens [CLS], [SEP], [PAD]\n",
    "    mask_arr = (rand < 0.15) \\\n",
    "             * (inputs.input_ids != 101) \\\n",
    "             * (inputs.input_ids != 102) \\\n",
    "             * (inputs.input_ids != 0)\n",
    "    \n",
    "    selection = []\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "    \n",
    "    # For each row in the input_ids, assign 103 [MASK] token to the selection indices\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        inputs.input_ids[i, selection[i]] = 103\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "426dc025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {key: torch.tensor(value[index].clone().detach()) for key, value in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cda45f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForPreTraining.from_pretrained(MODEL_NAME)\n",
    "\n",
    "with open(DATASET_PATH, 'r') as fp:\n",
    "    text = fp.read().split('\\n')\n",
    "    \n",
    "# Generate NSP Dataset\n",
    "sentence_a, sentence_b, label = generate_nsp_dataset(text)\n",
    "\n",
    "inputs = tokenizer(sentence_a,\n",
    "                   sentence_b,\n",
    "                   return_tensors='pt',\n",
    "                   max_length=MAX_LENGTH,\n",
    "                   truncation=True,\n",
    "                   padding='max_length')\n",
    "\n",
    "inputs['next_sentence_label'] = torch.LongTensor([label]).T\n",
    "\n",
    "# Generate MLM Dataset\n",
    "inputs = generate_mlm_dataset(tokenizer, inputs)\n",
    "\n",
    "dataset = BertDataset(inputs)\n",
    "loader  = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0865070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/159 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Epoch 0: 100%|██████████| 159/159 [00:49<00:00,  3.23it/s, loss=0.537]\n",
      "Epoch 1: 100%|██████████| 159/159 [00:49<00:00,  3.22it/s, loss=0.266]\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # this just creates blank progress bars for both our epochs\n",
    "    loop = tqdm(loader, leave=True) #leave=True enables us to see the progress bars for each epoch\n",
    "    for batch in loop:\n",
    "        # zero_grad sets the gradients of all optimized tensors to zero\n",
    "        # we need to set the gradients to zero b/f starting backprop b/c pytoch accumulates the gradients on subsequent backward pass (this is convenient while training RNNs)\n",
    "        # \n",
    "        optimizer.zero_grad() # stops the gradient calculations from the previous set being carried over to the next set\n",
    "        \n",
    "        # batch in loop contains the 4 inputs (ie. input_ids, token_ids, attention_masks and labels)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        next_sentence_label = batch['next_sentence_label'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # forward pass, feeding input data through all the neurons in the network from first to last layer\n",
    "        outputs = model(input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        next_sentence_label=next_sentence_label,\n",
    "                        labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward propagation, compute the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # update model parameters and take a step using the computerd gradient\n",
    "        optimizer.step()\n",
    "        loop.set_description(\"Epoch {}\".format(epoch))\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "model.save_pretrained(PRE_TRAINED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fba450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
